# Tutorial: Usando M√∫ltiplos Modelos no AXON

Este tutorial passo-a-passo mostra como configurar, treinar e comparar m√∫ltiplos modelos de machine learning no AXON.

## Pr√©-requisitos

- AXON instalado e configurado
- Dados de treinamento preparados
- Ambiente Python com depend√™ncias instaladas

## Passo 1: Configura√ß√£o Inicial

### 1.1 Configurar axon.cfg.yml

Edite o arquivo `axon.cfg.yml` para incluir m√∫ltiplos modelos:

```yaml
# Modelos para treinamento
models:
  train: ["lightgbm", "xgboost", "catboost", "lstm"]

  # Configura√ß√µes individuais
  lightgbm:
    n_estimators: 100
    learning_rate: 0.05
    num_leaves: 31
    max_depth: -1

  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8

  catboost:
    iterations: 500
    learning_rate: 0.05
    depth: 6
    verbose: False

  lstm:
    hidden_size: 64
    num_layers: 2
    sequence_length: 20
    batch_size: 32
    epochs: 50

# Otimiza√ß√£o habilitada
optimization:
  enabled: true
  n_trials: 50
  models: ["lightgbm", "xgboost", "catboost"]
```

### 1.2 Verificar Instala√ß√£o

Execute o teste b√°sico:

```bash
python -c "
from src.models import ModelRegistry
from src.utils import load_config
config = load_config()
registry = ModelRegistry(config)
print('‚úÖ Configura√ß√£o v√°lida!')
print('Modelos dispon√≠veis:', registry.list_available_models())
"
```

## Passo 2: Prepara√ß√£o de Dados

### 2.1 Executar Pipeline de Dados

```bash
# Preparar dados
python -m src.dataset

# Verificar dados gerados
ls -la data/processed/
```

### 2.2 Carregar e Explorar Dados

```python
import pandas as pd
import numpy as np
from pathlib import Path

# Carregar dados processados
train_df = pd.read_parquet('data/processed/train_features.parquet')
val_df = pd.read_parquet('data/processed/validation_features.parquet')

print(f"Train shape: {train_df.shape}")
print(f"Validation shape: {val_df.shape}")

# Verificar distribui√ß√£o de labels
target_col = 'y'
print("Distribui√ß√£o de labels (treino):")
print(train_df[target_col].value_counts(normalize=True))

# Verificar features dispon√≠veis
feature_cols = [col for col in train_df.columns if col not in ['timestamp', target_col]]
print(f"N√∫mero de features: {len(feature_cols)}")
print("Primeiras features:", feature_cols[:5])
```

## Passo 3: Treinamento B√°sico de M√∫ltiplos Modelos

### 3.1 Treinar Modelos Individualmente

```python
from src.models import ModelRegistry, train_model, save_model
from src.utils import load_config

# Carregar configura√ß√£o
config = load_config()
registry = ModelRegistry(config)

# Preparar dados
X_train = train_df[feature_cols]
y_train = train_df[target_col]
X_val = val_df[feature_cols]
y_val = val_df[target_col]

# Modelos para treinar
models_to_train = ['lightgbm', 'xgboost', 'catboost']

# Dicion√°rio para armazenar resultados
trained_models = {}
model_metrics = {}

print("üöÄ Iniciando treinamento de m√∫ltiplos modelos...")

for model_name in models_to_train:
    print(f"\n{'='*50}")
    print(f"üèÉ Treinando {model_name.upper()}")
    print(f"{'='*50}")

    try:
        # Obter modelo
        model = registry.get_model(model_name)

        # Treinar
        trained_model, metrics = train_model(
            model, X_train, y_train, X_val, y_val, model_name, config
        )

        # Salvar modelo
        model_path = save_model(
            trained_model, model_name, metrics, feature_cols, config
        )

        # Armazenar resultados
        trained_models[model_name] = trained_model
        model_metrics[model_name] = metrics

        print(f"‚úÖ {model_name} treinado com sucesso!")
        print(".4f"        print(".4f"        print(".4f"
    except Exception as e:
        print(f"‚ùå Erro ao treinar {model_name}: {e}")
        continue

print(f"\n‚úÖ Treinamento conclu√≠do! {len(trained_models)} modelos treinados.")
```

### 3.2 Comparar Performance

```python
import pandas as pd

# Criar DataFrame de compara√ß√£o
comparison_df = pd.DataFrame(model_metrics).T
print("\nüìä Compara√ß√£o de Modelos:")
print(comparison_df.round(4))

# Encontrar melhor modelo
best_model = comparison_df['accuracy'].idxmax()
best_accuracy = comparison_df.loc[best_model, 'accuracy']

print(f"\nüèÜ Melhor modelo: {best_model} (Accuracy: {best_accuracy:.4f})")

# Salvar compara√ß√£o
comparison_df.to_csv('outputs/artifacts/model_comparison.csv')
print("üíæ Compara√ß√£o salva em outputs/artifacts/model_comparison.csv")
```

## Passo 4: Otimiza√ß√£o de Hiperpar√¢metros

### 4.1 Executar Otimiza√ß√£o Autom√°tica

```python
from src.optimization import OptimizationEngine

# Inicializar otimizador
opt_engine = OptimizationEngine(config)

# Modelos para otimizar
models_to_optimize = ['lightgbm', 'xgboost']

optimized_models = {}

for model_name in models_to_optimize:
    print(f"\nüéØ Otimizando {model_name.upper()}...")

    try:
        # Executar otimiza√ß√£o
        results = opt_engine.optimize_model(
            model_name, X_train, y_train, X_val, y_val
        )

        # Treinar modelo final com melhores par√¢metros
        best_params = results['best_params']

        model = registry.get_model(model_name, **best_params)
        final_model, final_metrics = train_model(
            model, X_train, y_train, X_val, y_val, f"{model_name}_optimized", config
        )

        # Salvar modelo otimizado
        model_path = save_model(
            final_model, f"{model_name}_optimized", final_metrics, feature_cols, config
        )

        optimized_models[model_name] = {
            'model': final_model,
            'metrics': final_metrics,
            'best_params': best_params,
            'optimization_results': results
        }

        print(f"‚úÖ {model_name} otimizado!")
        print(".4f"
    except Exception as e:
        print(f"‚ùå Erro na otimiza√ß√£o de {model_name}: {e}")
        continue
```

### 4.2 Comparar Modelos Otimizados vs. Padr√£o

```python
# Comparar otimizados com vers√µes padr√£o
comparison_data = {}

for model_name in models_to_optimize:
    if model_name in model_metrics and model_name in optimized_models:
        comparison_data[f"{model_name}_base"] = model_metrics[model_name]
        comparison_data[f"{model_name}_optimized"] = optimized_models[model_name]['metrics']

comparison_opt_df = pd.DataFrame(comparison_data).T
print("\nüìä Compara√ß√£o: Base vs Otimizado")
print(comparison_opt_df[['accuracy', 'f1', 'precision']].round(4))
```

## Passo 5: Treinar Modelo Ensemble

### 5.1 Configurar Ensemble

```yaml
# Adicionar ao axon.cfg.yml
ensemble:
  ensemble_type: 'weighted'
  combination_strategy: 'performance_based'
  base_models: ['lightgbm', 'xgboost', 'catboost']
  voting_type: 'soft'
  cv_folds: 5
  regime_detection: true
  regime_window: 50
```

### 5.2 Treinar Ensemble

```python
# Treinar ensemble
ensemble_config = config.get('models', {}).get('ensemble', {})
ensemble = registry.get_model('ensemble', **ensemble_config)

print("üèóÔ∏è Treinando Ensemble...")
ensemble_model, ensemble_metrics = train_model(
    ensemble, X_train, y_train, X_val, y_val, 'ensemble', config
)

# Salvar ensemble
ensemble_path = save_model(
    ensemble_model, 'ensemble', ensemble_metrics, feature_cols, config
)

print("‚úÖ Ensemble treinado!")
print(".4f"
```

### 5.3 Comparar com Modelos Individuais

```python
# Adicionar ensemble √† compara√ß√£o
all_metrics = model_metrics.copy()
all_metrics['ensemble'] = ensemble_metrics

final_comparison = pd.DataFrame(all_metrics).T
print("\nüèÜ Compara√ß√£o Final (Incluindo Ensemble):")
print(final_comparison[['accuracy', 'f1', 'precision', 'auc']].round(4))

# Comparar ensemble vs melhor individual
individual_best = final_comparison.drop('ensemble')['accuracy'].max()
ensemble_accuracy = final_comparison.loc['ensemble', 'accuracy']

if ensemble_accuracy > individual_best:
    print("‚úÖ Ensemble melhorou a performance!")
else:
    print("‚ö†Ô∏è Ensemble n√£o melhorou significativamente")
```

## Passo 6: An√°lise de Features e Interpretabilidade

### 6.1 Import√¢ncia de Features

```python
from src.models import get_feature_importance

# Analisar import√¢ncia para cada modelo
feature_importance = {}

for model_name, model in trained_models.items():
    try:
        importance_df = get_feature_importance(model, feature_cols, model_name)
        feature_importance[model_name] = importance_df

        print(f"\nüîç Top 5 features - {model_name}:")
        for idx, row in importance_df.head(5).iterrows():
            print(".4f"
        # Salvar import√¢ncia
        importance_df.to_csv(f'outputs/artifacts/{model_name}_feature_importance.csv', index=False)

    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao calcular import√¢ncia para {model_name}: {e}")
```

### 6.2 Correla√ß√£o entre Previs√µes

```python
# Calcular previs√µes de valida√ß√£o para todos os modelos
predictions = {}

for model_name, model in trained_models.items():
    try:
        if hasattr(model, 'predict_proba'):
            pred_proba = model.predict_proba(X_val)[:, 1]
        else:
            pred = model.predict(X_val)
            pred_proba = pred.astype(float)

        predictions[model_name] = pred_proba
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao fazer previs√µes com {model_name}: {e}")

# Calcular matriz de correla√ß√£o
pred_df = pd.DataFrame(predictions)
correlation_matrix = pred_df.corr()

print("\nüìà Correla√ß√£o entre previs√µes dos modelos:")
print(correlation_matrix.round(4))

# Correla√ß√£o baixa = diversidade boa para ensemble
avg_correlation = correlation_matrix.mean().mean()
print(".4f"
if avg_correlation < 0.8:
    print("‚úÖ Boa diversidade entre modelos!")
else:
    print("‚ö†Ô∏è Modelos muito correlacionados - ensemble pode n√£o ajudar muito")
```

## Passo 7: Valida√ß√£o Cruzada

### 7.1 Executar CV para Robustez

```python
from src.models import cross_validate_model

# Executar CV para os melhores modelos
cv_results = {}

for model_name in ['lightgbm', 'xgboost', 'ensemble']:
    if model_name in trained_models or model_name == 'ensemble':
        model = trained_models.get(model_name, ensemble_model)

        print(f"\nüîÑ Valida√ß√£o Cruzada - {model_name}...")
        cv_metrics = cross_validate_model(model, X_train, y_train, cv=5)

        cv_results[model_name] = cv_metrics

        print(f"  Accuracy: {cv_metrics['cv_accuracy_mean']:.4f} ¬± {cv_metrics['cv_accuracy_std']:.4f}")
        print(f"  F1: {cv_metrics['cv_f1_mean']:.4f} ¬± {cv_metrics['cv_f1_std']:.4f}")
```

## Passo 8: Avalia√ß√£o Final

### 8.1 Executar Avalia√ß√£o

```bash
# Executar avalia√ß√£o completa
python -m src.evaluate

# Ou especificamente para um modelo
python -c "
from src.evaluate import run_evaluation
from src.models import load_model

# Carregar melhor modelo
model, _ = load_model('outputs/artifacts/ensemble_20250101_120000.pkl')

# Executar avalia√ß√£o
results = run_evaluation(model, config)
print('Evaluation results:', results)
"
```

### 8.2 Analisar Resultados

```python
# Carregar resultados de avalia√ß√£o
import json

with open('outputs/metrics/EV_ensemble_20250101_120000.json', 'r') as f:
    eval_results = json.load(f)

print("üìä Resultados de Avalia√ß√£o:")
print(f"  Accuracy: {eval_results.get('accuracy', 0):.4f}")
print(f"  Precision: {eval_results.get('precision', 0):.4f}")
print(f"  Recall: {eval_results.get('recall', 0):.4f}")
print(f"  F1-Score: {eval_results.get('f1_score', 0):.4f}")
```

## Passo 9: Relat√≥rio Final

### 9.1 Gerar Relat√≥rios

```bash
# Gerar relat√≥rio completo
python -m src.report
```

### 9.2 Resumo Executivo

```python
print("="*60)
print("üìã RESUMO EXECUTIVO - MULTI-MODEL AXON")
print("="*60)

print(f"üìä Modelos treinados: {len(trained_models)}")
print(f"üéØ Melhor modelo individual: {best_model}")
print(f"üèÜ Accuracy individual: {best_accuracy:.4f}")

if 'ensemble' in all_metrics:
    ensemble_accuracy = all_metrics['ensemble']['accuracy']
    improvement = ((ensemble_accuracy - best_accuracy) / abs(best_accuracy)) * 100
    print(f"üé≠ Ensemble Accuracy: {ensemble_accuracy:.4f}")
    print(f"üìà Melhoria do Ensemble: {improvement:.1f}%")

print(f"‚è±Ô∏è  Tempo total de treinamento: ~{(len(trained_models) * 2 + len(optimized_models) * 10):.0f} minutos")
print(f"üíæ Modelos salvos em: outputs/artifacts/")
print(f"üìà Relat√≥rios em: outputs/reports/")

print("\n‚úÖ Workflow conclu√≠do com sucesso!")
print("Pr√≥ximos passos:")
print("  1. Revisar relat√≥rios em outputs/reports/")
print("  2. Ajustar par√¢metros se necess√°rio")
print("  3. Considerar deploy em produ√ß√£o")
print("  4. Monitorar performance ao vivo")
```

## Dicas Avan√ßadas

### Paraleliza√ß√£o de Treinamento

```python
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

def train_single_model(model_name):
    # L√≥gica de treinamento para um modelo
    pass

# Treinar em paralelo
with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
    results = executor.map(train_single_model, models_to_train)
```

### Monitoramento de Recursos

```python
import psutil
import GPUtil

def log_system_resources():
    # CPU e mem√≥ria
    cpu_percent = psutil.cpu_percent()
    memory_percent = psutil.virtual_memory().percent

    # GPU
    gpus = GPUtil.getGPUs()
    gpu_memory = gpus[0].memoryUsed if gpus else 0

    print(f"CPU: {cpu_percent}%, RAM: {memory_percent}%, GPU: {gpu_memory}MB")

# Log durante treinamento
log_system_resources()
```

### Configura√ß√µes por Cen√°rio

```python
# Para desenvolvimento r√°pido
dev_config = {
    'lightgbm': {'n_estimators': 50, 'learning_rate': 0.1},
    'xgboost': {'n_estimators': 50, 'max_depth': 4}
}

# Para produ√ß√£o
prod_config = {
    'lightgbm': {'n_estimators': 200, 'learning_rate': 0.05},
    'xgboost': {'n_estimators': 200, 'max_depth': 8}
}
```

## Troubleshooting Comum

### Modelo n√£o converge
```python
# Aumentar learning rate
config['models']['lightgbm']['learning_rate'] = 0.1

# Ou reduzir regulariza√ß√£o
config['models']['lightgbm']['reg_alpha'] = 0.0
```

### Mem√≥ria insuficiente
```python
# Reduzir batch size para LSTM
config['models']['lstm']['batch_size'] = 16

# Ou reduzir sequence length
config['models']['lstm']['sequence_length'] = 10
```

### Overfitting
```python
# Aumentar regulariza√ß√£o
config['models']['xgboost']['reg_alpha'] = 0.1
config['models']['xgboost']['reg_lambda'] = 1.0

# Ou reduzir complexidade
config['models']['lightgbm']['num_leaves'] = 20
```

---

**üéâ Parab√©ns!** Voc√™ completou o tutorial de m√∫ltiplos modelos no AXON.

Para pr√≥ximos passos, consulte:
- [Guia de Configura√ß√£o](./configuration-guide.md)
- [API Reference](../api/models-api.md)
- [Demo Interativo](../demo/model-comparison.ipynb)